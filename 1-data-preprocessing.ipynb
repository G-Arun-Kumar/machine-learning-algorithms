{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8167649,"sourceType":"datasetVersion","datasetId":4833304}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/iamarunkumar/1-data-preprocessing?scriptVersionId=178198074\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **Feature Scaling**\n\nFeature scaling is the process of **scaling the data**. It simply consists of scaling all our variables or all our features actually to make sure they **all take values in the same scale**. And we do this, so as to prevent one feature dominating the other feature that would be neglected by the future machine learning model. For example, if there are columns like salary and age -->The values of salary column ($ 10,000)will be enoromously differed from age (45 yrs). Here 10,000 is a huge difference with 45 yrs. Machine learning algorithm won't work well with unscaled data. So, we need to scale the data before we apply our algorithm technique/work on model.\n\n**Feature scaling is always applied to columns (individual columns) and not acorss all columns (rows).**\n\nThere are two main types of feature scaling. They are Normalization & Standardization.\n\nNormalization => X - Xmin / Xmax - Xmin. Usually the value ranges between 0 and 1.\n\nStandardization => X - mu / sigma. Usually the value ranges between -3 and +3.\n\n**Always remember, feature scaling is applied after splitting the training set and test set**","metadata":{}},{"cell_type":"markdown","source":"There are **6 steps in data preprocessing** to be followed in sequential manner. They are,\n\n1. Import the required libraries\n2. Import the data set\n3. Taking care of missing data\n4. Encoding categorical data\n5. Splitting the dataset into training set and test set\n6. Feature scaling","metadata":{}},{"cell_type":"markdown","source":"# Import the required libraries","metadata":{}},{"cell_type":"code","source":"# Import the required libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-23T17:27:09.821157Z","iopub.execute_input":"2024-04-23T17:27:09.821563Z","iopub.status.idle":"2024-04-23T17:27:11.398355Z","shell.execute_reply.started":"2024-04-23T17:27:09.821533Z","shell.execute_reply":"2024-04-23T17:27:11.396905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import the datasets","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/product-purchase/Data.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:11.400412Z","iopub.execute_input":"2024-04-23T17:27:11.401135Z","iopub.status.idle":"2024-04-23T17:27:11.422565Z","shell.execute_reply.started":"2024-04-23T17:27:11.401098Z","shell.execute_reply":"2024-04-23T17:27:11.421281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create two new entities.**\n\n    1.matrix of features \n    2.Dependent variable vector\n    \n1. **Matrix of features**: Features are the columns with which we are going to predict the dependent variable.\n   They are also called as independent variables.\n\n2. **Dependent variables**: Mostly these columns will be the last column in your dataset. This is the column where the prediction will be done based on the informations available in features column.","metadata":{}},{"cell_type":"code","source":"\"\"\"Creating feature entity X\niloc is the function in pandas and stands for 'locate indexes' which will take \nthe indexes of the rows & columns we want to extract from dataset. We always start with rows to extract and then to column\n\nWe specify ':' to extract all rows. Becuase ':' means range and when specifying a range without upper bound or \nlower bound means we are taking everything in range.\n\n Creating matrix of features\"\"\"\nX = df.iloc[:,:-1].values\n\n# Creating vector entity Y\ny = df.iloc[:,-1].values","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:11.424014Z","iopub.execute_input":"2024-04-23T17:27:11.42436Z","iopub.status.idle":"2024-04-23T17:27:11.43804Z","shell.execute_reply.started":"2024-04-23T17:27:11.424331Z","shell.execute_reply":"2024-04-23T17:27:11.436405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:11.44024Z","iopub.execute_input":"2024-04-23T17:27:11.440854Z","iopub.status.idle":"2024-04-23T17:27:11.449962Z","shell.execute_reply.started":"2024-04-23T17:27:11.440815Z","shell.execute_reply":"2024-04-23T17:27:11.448928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:11.451274Z","iopub.execute_input":"2024-04-23T17:27:11.451587Z","iopub.status.idle":"2024-04-23T17:27:11.462534Z","shell.execute_reply.started":"2024-04-23T17:27:11.451561Z","shell.execute_reply":"2024-04-23T17:27:11.46154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Taking care of missing values\n\nOur dataset should always be clean while training our models.It must be free from missing values.\nThere are several ways we remove missing values. Either we remove the row of missing values until the whole data is 1!\nThe classic way is replacing the missing value with the average/mean value of that column.","metadata":{}},{"cell_type":"code","source":"# We do this by the famous data science library called sklearn (scikit learn)\n# SimpleImputer is the class from 'impute' module in sklearn library helps to replace the missing values\n\nfrom sklearn.impute import SimpleImputer\n# Assign object to SimpleImputer class called Imputer\n# SimpleImputer has 2 arguments. 'missing_values' will be assigned with np.nan and 'strategy' as mean\nimputer = SimpleImputer(missing_values=np.nan,strategy='mean')\n\n\"\"\"Now, time to connect the object to matrix of features. This is done using fit method.\nThe fit method will just look for all missing variables and compute the average of all values in the column.\nRemember,we need to pass only numerical values to find the average and not string column. Be careful in passing argument \nHere we pass all rows ([:]) and numerical columns only ([1:3])\"\"\"\nimputer.fit(X[:,1:3])\n\n\"\"\"Now, time to replace all missing values with average of those column values. This is achieved by 'transform' method\nRemember, transform method will look for arguments on which columns the missing values should be replaced with.\nSo, just pass the same arguments that are passed in fit method. becuase that is where we need to replace the values\"\"\"\n\nX[:,1:3] = imputer.transform(X[:,1:3])","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:11.464238Z","iopub.execute_input":"2024-04-23T17:27:11.464578Z","iopub.status.idle":"2024-04-23T17:27:13.755498Z","shell.execute_reply.started":"2024-04-23T17:27:11.46455Z","shell.execute_reply":"2024-04-23T17:27:13.753428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:13.758022Z","iopub.execute_input":"2024-04-23T17:27:13.759493Z","iopub.status.idle":"2024-04-23T17:27:13.76909Z","shell.execute_reply.started":"2024-04-23T17:27:13.75941Z","shell.execute_reply":"2024-04-23T17:27:13.766911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding categorical data\n\nIt will be difficult to compute the columns with strings and columns with numbers or compute matrix of features with dependent variable. Machine learning model requires all columns in numerics. So, we need to change all the strings to numbers.\n\nWe achieve this by a process called **one hot encoding**. One hot encoing consits of turning the country column (from our data set) into **3 columns**. If there are 5 differnt classes/different countries, then we turn into 5 different columns.\n\n**If we have 'n' no. of categories, we use OneHotEncoder() and if we have 2 types of classes (yes/no) we use LabelEncoder()**\n\n**Remember, for categorical columns (more than 2 category) we use OneHotEncoder() and for labelled columns (2 classes/types) we use LabelEncoder()**\n\nOne hot encoding consists of creating binary vectors for each of these countries.\nThere are two types of encoding done. One is for independent variable in features/categorical column and the other encoding done for dependent variable or labels (non-numerical values).","metadata":{}},{"cell_type":"markdown","source":"**Encoding independent variable**\n\nWe use 2 classes to encode the independent variable. they are,\n1. ColumnTransformer class from compose module in sklearn library\n2. OneHotEncoder class from preprocessing module in sklearn library","metadata":{}},{"cell_type":"code","source":"# Let's start importing the ColumnTransformer class and OneHotEncoder class\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:13.774417Z","iopub.execute_input":"2024-04-23T17:27:13.775119Z","iopub.status.idle":"2024-04-23T17:27:13.823101Z","shell.execute_reply.started":"2024-04-23T17:27:13.775052Z","shell.execute_reply":"2024-04-23T17:27:13.821774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Creating object 'ct' to ColumnTransformer class\nColumnTransformer has 2 arguments-Transformers(what kind of transformation) & Remainder(columns that will not transform)\nTransformer argument has 3 things-1. what kind of transformation (encoding) 2.what kind of encoding (onehotencoding)\n3.indexes of column need to be encoded. We do all these by a pair of square brackets and parenthesis.\n\n\nBe careful and look into the transformer argument how the values are passed in [(),[]].\nThree tuple values are passed inside [] and additional [0] for the index of column for one hot encoding.\n\nThe remainder argument has a value called 'passthrough' which is a code that let the model know that the other\ntwo columns (Age & Salary) should be left with features and not to be encoded.\"\"\"\nct = ColumnTransformer(transformers=[('encoding',OneHotEncoder(),[0])], remainder='passthrough')\n\n\"\"\"Now, connect the object to the matrix of features. this time this is done in straight process as ColumnTransformer class\nhas fittransform method. It will need the argument as X(matrix of features). The output of fittransform will be the\nmatrix of features inside which the one hot encoding done with three columns.\"\"\"\n\nX = np.array(ct.fit_transform(X))\n# It's expected in the future machine learning model that X (matrix of features) to be numpy array. And hence we passed\n# np.array(ct.fit_transform(X))","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:13.824776Z","iopub.execute_input":"2024-04-23T17:27:13.825265Z","iopub.status.idle":"2024-04-23T17:27:13.851454Z","shell.execute_reply.started":"2024-04-23T17:27:13.825231Z","shell.execute_reply":"2024-04-23T17:27:13.850084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:13.855367Z","iopub.execute_input":"2024-04-23T17:27:13.856101Z","iopub.status.idle":"2024-04-23T17:27:13.876268Z","shell.execute_reply.started":"2024-04-23T17:27:13.856051Z","shell.execute_reply":"2024-04-23T17:27:13.875318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Encoding dependent variable**\n\nWe use a class called LabelEncoder from preprocessing module which will encode the labels ('Yes','No') into zeros & ones.\nFor two types of classes, we use label encoder.","metadata":{}},{"cell_type":"code","source":"\"\"\"Let's import LabelEncoder() class from preprocessing module of sklearn to encode the labelled classes into '0s' and '1s'\nNote that, we don't need to convert the output to numpy array as expected in OneHotEncoder() in binary format. We don't\nconvert the labelled column to binary format.\"\"\"\n\nfrom sklearn.preprocessing import LabelEncoder\n\n#Remember, We don't need to pass any arguments for LabelEncoder() class\n\nle = LabelEncoder()\ny = le.fit_transform(y)\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:13.877591Z","iopub.execute_input":"2024-04-23T17:27:13.878139Z","iopub.status.idle":"2024-04-23T17:27:13.908571Z","shell.execute_reply.started":"2024-04-23T17:27:13.878102Z","shell.execute_reply":"2024-04-23T17:27:13.907336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the dataset into Training set and Test set\nSplitting dataset into training set and test set means, it consists of 2 different set- **1 training set** which we will use to **train the machine learning model on existing observations** and **1 test set** where we will **evaluate the performance of our machine learning model on new observations**.\n\n**Remember the most important point - We do 'feature scaling' after splitting the data set into training set and test set**\nThe reason being is the test set simply means the brand new set on which we will evaluate the performance of ML model. Then, after training the model, we will deploy the model in test set. So this means, we are not supposed to work with for the training. \n\nAnd feature scaling is the technique which we use mean and standard deviation of features to scale the data. If we apply feature scaling before the split, then all data will get mean and standard deviation including the test set. This we call as information leakage on test set. This is wrong. We musn't disturb the test set. It must be fresh/brand new.\n\nSo to prevent the information leakage on test set, we do feature scaling after splitting the training set and test set. **Hence, we apply feature scaling after splitting the training set and test set**\n\n**Splitting the data set is done using method called 'train_test_split' in 'method selection' module from sklearn library**\nAnd we create a pair of matrix of features and dependent variable vectors for training set and another pair of matrix of features and dependent variable vectors for test set.\n\n**Remember, we create 4 seperate sets. They are,**\n\n1. X_train - matrix of features on training set\n2. X_test - matrix of features of test set\n3. y_train - dependent variable vectors of training set\n4. y_test - dependent variable vector of test set\n\nFuture machine learning model expect these 4 splits as inputs. For training, it will expect Xtrain & ytrain as input in a method called fit method and for predictions also called as inference, these model will predict Xtest\n","metadata":{}},{"cell_type":"code","source":"# Let's import the method 'train_test_split' from module called 'model selection' in sklearn library\n\nfrom sklearn.model_selection import train_test_split\n\n\"\"\"We must use the same order of X_train, X_test, y_train and y_test.\nWe need to pass 4 parameters in train_test_split function. They are actually,\n1. The matrix of features X\n2. dependent variable vector y\n3. split size or the test size of the data set that we split the ratio of training & test. We pass 0.2 \nmeaning 20% observation for test set and 80% observation for training the model\n4. random_state is the argument that needs to maintain the randomness of data that is planned to split and we pass as 1.\nWe are fixing the seed as 1 for the same training and test set\nRemember splitting the data will not be in order. It will be randomly splitted. hence random_state\"\"\"\n\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:13.910131Z","iopub.execute_input":"2024-04-23T17:27:13.910566Z","iopub.status.idle":"2024-04-23T17:27:13.942605Z","shell.execute_reply.started":"2024-04-23T17:27:13.910529Z","shell.execute_reply":"2024-04-23T17:27:13.941099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Now, let's start printing all 4 split sets in each individual cell. Note that we have only 8 rows (8 customers\ntaken randomly from data set) in X_train since we passed the test_size as 0.2\n\nAnd we clearly recognize the features as 3 columns since we had done one hot encoding for country (categorical) column\nRemember we have only matrix of features (3 columns) since it's X_train.\"\"\"\n\nprint(X_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:13.944939Z","iopub.execute_input":"2024-04-23T17:27:13.945652Z","iopub.status.idle":"2024-04-23T17:27:13.95581Z","shell.execute_reply.started":"2024-04-23T17:27:13.945609Z","shell.execute_reply":"2024-04-23T17:27:13.954376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here, you can see only two rows of matrix of features selected randomly since we passed the test_size as 0.2\nprint(X_test) ","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:13.957924Z","iopub.execute_input":"2024-04-23T17:27:13.958978Z","iopub.status.idle":"2024-04-23T17:27:13.971254Z","shell.execute_reply.started":"2024-04-23T17:27:13.958925Z","shell.execute_reply":"2024-04-23T17:27:13.969681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Very important point below - we have 8 label encoded values that are same corresponding to X_train (training set).\nI mean, the label encoded values (dependent variables) are from the same row \nof one training set of one hot encoded values (matrix of features).\"\"\"\n\nprint(y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:13.973272Z","iopub.execute_input":"2024-04-23T17:27:13.974043Z","iopub.status.idle":"2024-04-23T17:27:13.984897Z","shell.execute_reply.started":"2024-04-23T17:27:13.973999Z","shell.execute_reply":"2024-04-23T17:27:13.983382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Again, here we have 2 values that are corresponding to the same test set of matrix of features\nprint(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:13.986516Z","iopub.execute_input":"2024-04-23T17:27:13.987871Z","iopub.status.idle":"2024-04-23T17:27:14.007343Z","shell.execute_reply.started":"2024-04-23T17:27:13.987822Z","shell.execute_reply":"2024-04-23T17:27:14.004054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Scaling\n\nThere are enough information given for feature scaling. Refer the first topic for better clarity.\n\nRemember, standardisation and normalisation are the two main feature scaling technique that put all features in the same scale.\n\nAlso, **standardisation will put all our values between -3 and +3**\n        **Normalisation will put all our values between 0 and +1**\n        \n**Normalization** => X - Xmin / Xmax - Xmin.\n\n**Standardization** => X - mu / sigma. \n\nNow here, we will confirm which type of feature scaling is good for ML model evaluation. Either normalisation or standardisation. Below is the answer.\n\n**Normalisation** is recommended when we have data normally distributed or normal distributions in most of the features.\n**Standardisation** works well in all cases and since normalisation has some specific conditions when the features are normally\ndistributed, the **recommendation is to have standardisation technique followed**.\n\nFeature scaling is done using a **class** called **StandardScaler()** in preprocessing module of sklearn library.\n\nYet, **another question to answer** - do we need to apply feature scaling or standardisation to dummy variables (category column which is one hot encoded) in matrix of features. below is the answer.\n\nThe answer is **No**. because the feature scaling is the scaling of features all in same range. standardisation will have values -3 and +3 and the dummy variables already have the values less than that. so **standardisation will worsen the data if we apply the feature scaling to those matrix of features**.\n\nAlso, we will loose information on which dummy variable (1.0 0.0 0.0) belongs to which country (France) if we apply standardisation to those variables.\n\n**Finally, to keep the interpretability of data, we don't apply standardisation to dummy variables.**\n","metadata":{}},{"cell_type":"code","source":"# Let's start importing the Standard Scaler class from preprocessing module in sklearn library\n\nfrom sklearn.preprocessing import StandardScaler\n\"\"\"Remember, we don't need to pass any argument on this class because it does the direct job of identifying mean and\nstandard deviation and applying it in formula.\"\"\"\n\nsc = StandardScaler()\n\"\"\"Now, let's apply standardisation only to columns ignoring the dummy variables for training set.\nRemember while looking into the column range now, don't look at the data. look into the output of X. Since the\none hot encoded done (column will be splitted into 3) the range of column will now be changed. It starts from 3\nbecause the index from 0 to 2 will have dummy variable and we ignore these columns. So we start from range 3.\nRemember we always fetch all rows so we use ':' for selecting all rows.\n\nFit will just compute the mean and standard deviation of all the values or features.\nTransform will transform all the values into the one which has in formula (X-mean/standard deviation)\n\nHere, we pass the arguments as 'X_train[:,3:]' since we feature scale only onto those columns\"\"\"\n\nX_train[:,3:] = sc.fit_transform(X_train[:,3:])\n\n\"\"\"now, we use only transform method for X_test because indeed the features of test set should be scaled by the same scaler\nthat is done in training set. Since, X_test is like a brand new or production data, we do this.\n\nIf we spply fit_transform() to X_test, then we will get a new scaler which is not making sense.\"\"\"\n\nX_test[:,3:] = sc.transform(X_test[:,3:])","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:14.010421Z","iopub.execute_input":"2024-04-23T17:27:14.012615Z","iopub.status.idle":"2024-04-23T17:27:14.027337Z","shell.execute_reply.started":"2024-04-23T17:27:14.012548Z","shell.execute_reply":"2024-04-23T17:27:14.026074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:14.029014Z","iopub.execute_input":"2024-04-23T17:27:14.030424Z","iopub.status.idle":"2024-04-23T17:27:14.038731Z","shell.execute_reply.started":"2024-04-23T17:27:14.030372Z","shell.execute_reply":"2024-04-23T17:27:14.037315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T17:27:14.04105Z","iopub.execute_input":"2024-04-23T17:27:14.041983Z","iopub.status.idle":"2024-04-23T17:27:14.053247Z","shell.execute_reply.started":"2024-04-23T17:27:14.041935Z","shell.execute_reply":"2024-04-23T17:27:14.051496Z"},"trusted":true},"execution_count":null,"outputs":[]}]}