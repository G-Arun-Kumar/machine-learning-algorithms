{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Association Rule Learning Intuition\n\n**'People who bought also bought'**. We would have come accross this statement or information when we buy in e-commerce websites like Flipkart or amazon. Yes. we are going to see today a topic about **recommender systems**. It's called as **association rule learning**.\n\nWe will see this via. a story.\n\nA **convenience store** did some analytics around the products that people were purchasing. They analyzed thousands and thousands and thousands of products. **Surprisingly, a peculiar commanilities between two different products** are found.\nThey are **diapers & beers**. Very peculiar ain't it?..\n\n**Deep into the story** - The husbands who are out of work and on their return to home, the wives calls them to buy diapers when they come home. And husbands after buying the diapers, they tend to buy beers before they move on to billing. This is the reason for these two products to get sold more.\n\nSo, In short, this algorithm helps to give the rule to buy two things with high statistical value. The products if bought together, will give good sales to business person.\n\nIn other words, it helps to identify the two best products that can be bought together using the rule of statistical approch and not just with speculation. business people will use criteria to increase sales of a product or do market by giving the ads to motivate the customer to buy this product.\n\n**People who bought something also bought something, people who watched also watched something, people who did something also did something** anything could be.\n\nAnd this ideally shouldn't stop **recommending with one other product alone. It can also use more than one. It can be three or four or five and so on**.\n\nThere are many types of algorithms available on association rule learning. Of which we will see two main types. They are,\n\n1. Apriori\n\n2. Eclat","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Apriori Intuition\n\nThere are some potential rules to recommend a product.\nApriori comes in three parts. They are,\n\n1. Support\n\n2. Confidence\n\n3. Lift\n\nLet's see everything in detail. The below diagram depicts the users who watched certain movies. There are people who watched movies1,2,3,4. There are people who watched both movies 1 and 4. There are people who watched both movies 1 and 3.\nPeople who both 2 and 4. People who watched 1 and 2 and so on and on and on... there are several combinations like this.\nSo, to apply the rule to recommend viewers to buy the other movie, the above algorithm is applied.\n\n**Support** - It's very similar to naive bayes algorithm. It is used for **movie recommendation**. **Support for movie M** is defined as,\n\n**Support(M) = No. of user watchlist containing M / Total no. of users.**\n\nThe above statement is as simple as the no. of users who watched a particular movie divided by the total no. of users.\n\n**Confidence(M) = No. of users watchlist containing M1 and M2 / No. of users watchlist containing M1**.\n\nThe above statement is nothing but, if suppose a movie M1 is watched by 50 users. And the same 50 users also watched movie M2 is 20. Then **confidence** is 20/50 = 2.5\n\n**Lift(M1->M2) = Confidence(M1->M2) / Support(M2)**\n\nThe above statement clearly defines. It's just the confidence/support.\n\nLet's see the step by step approach that above algorithm involves.\n\n**Step1** - Set a minimum support and confidence. This means - Since there are so many different recommendations and we have just seen 2 movies in our example. Apriori is a slow alogrithm, that goes through 100 different combinations. In our case it's 100 different movies. **We might not look at products that have a support less than 20%** we don't need to consider them because we don't want to waste time building a model for something that has a success rate of 20% on it's own.\n\n**Step2** - Take all subsets in transactions having higher support than minimum support.\n\n**Step3** - Take all rules of these subsets having higher confidence than minimum confidence.\n\n**Step4** - Sort the rules by decreasing it. - That's where the lift finally comes in. The rule with highest lift given these criteria is going to be strongest rule. This is the one we might want to look into first.","metadata":{}},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-06-06T00:52:39.296325Z","iopub.execute_input":"2024-06-06T00:52:39.297089Z","iopub.status.idle":"2024-06-06T00:52:39.303418Z","shell.execute_reply.started":"2024-06-06T00:52:39.297041Z","shell.execute_reply":"2024-06-06T00:52:39.302061Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Importing dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/market-basket-analysis/Market_Basket_Optimisation.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T00:58:00.456348Z","iopub.execute_input":"2024-06-06T00:58:00.456717Z","iopub.status.idle":"2024-06-06T00:58:00.487317Z","shell.execute_reply.started":"2024-06-06T00:58:00.456691Z","shell.execute_reply":"2024-06-06T00:58:00.486292Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}