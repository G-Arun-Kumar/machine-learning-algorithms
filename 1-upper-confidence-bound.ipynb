{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reinforcement Learning Intuition\n\n**Reinforcement learning is a subfield of machine learning where an agent learns to make decisions by performing actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the model is trained on a fixed dataset of input-output pairs, RL involves learning through interaction with the environment, with the aim of discovering the optimal actions that yield the highest reward over time.**\n\n**Key Components of Reinforcement Learning**\n\nAgent: The learner or decision-maker that interacts with the environment.\n\nEnvironment: Everything the agent interacts with and operates within.\n\nState (s): A representation of the current situation or configuration of the environment.\n\nAction (a): A decision or move made by the agent that affects the state.\n\nReward (r): Feedback from the environment, typically a scalar value, which the agent aims to maximize.\n\nPolicy (Ï€): A strategy used by the agent to decide which actions to take based on the current state. Policies can be deterministic or stochastic.\n\nValue Function (V): Estimates the expected cumulative reward from a given state, helping the agent assess the long-term benefit of states.\n\nQ-Value or Action-Value Function (Q): Estimates the expected cumulative reward from taking a particular action in a given state.\n\n**How Reinforcement Learning Works**\n\nInitialization: The agent starts with some initial policy or strategy.\n\nInteraction: The agent interacts with the environment by observing the state, taking an action, and receiving a reward.\n\nLearning: The agent updates its knowledge (policy, value functions) based on the received reward and observed transitions.\n\nIteration: Steps 2 and 3 are repeated, allowing the agent to learn and improve its policy over time.\n\n**Applications of Reinforcement Learning**\n\n**1.Robotics:**\n\nAutonomous navigation and manipulation tasks.\nExample: A robot learning to pick and place objects.\n\n**Gaming:**\n\nTraining AI to play and excel in games.\nExample: AlphaGo, which mastered the game of Go, and AlphaZero, which excelled in Go, Chess, and Shogi.\n\n**Finance:**\n\nPortfolio management, algorithmic trading, and risk management.\nExample: RL-based strategies for trading stocks and options.\n\n**Healthcare:**\n\nPersonalized treatment plans, medical diagnosis, and drug discovery.\nExample: Optimizing chemotherapy dosage for cancer treatment.\n\n**Examples of Reinforcement Learning in Action**\n\n**AlphaGo by DeepMind:**\n\nUtilized deep RL and Monte Carlo tree search to defeat human champions in the game of Go.\n\n**OpenAI Five:**\n\nTrained a team of AI agents to play the complex strategy game Dota 2, achieving superhuman performance.\n\n**Autonomous Helicopter Flight:**\n\nRL algorithms were used to teach helicopters to perform acrobatic maneuvers autonomously.\n\nAlthough we have various types of reinforcement algorithms like **model free algorithms, policy based algorithms, model based algorithms and advanced algorithms**\n\n**1. Model free algorithms** - Q-Learning, SARSA(State-Action-Reward-State-Action), Deep Q-Networks, Double DQN, Dueling DQN.\n\n**2. Policy based algorithms** - REINFORCE, Actor-Critic, Advantage Actor-Critic(A2C), Asynchronous Advantage Actor-Critic(A3C).\n\n**3. Model based algorithms** - Monte Carlo Tree Search(MCTS), AlphaZero\n\n**4. Advanced Algorithms** - Proximal Policy Optimization(PPO), Soft Actor-Critic(SAC), Trust Region Policy Optimization, Rainbow, TD3 (Twin Delayed Deep Deterministic Policy Gradient).\n\n**The multi armed bandit problem**\n\nOf all the above variants, today we are going to deal with 2 significant reinforcement learning algorithms particularly in the domain of **multi-armed bandit problems**. They are,\n\n1. Upper confidence Bound\n\n2. Thomson Sampling\n\nBoth Upper Confidence Bound and Thompson Sampling algorithms are essential tools in the arsenal of reinforcement learning, particularly for problems where the **primary challenge is balancing exploration and exploitation.** They are simple, effective, and provide a good foundation for understanding more complex RL algorithms.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}}]}